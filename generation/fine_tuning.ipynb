{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaroncui/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning with Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "The Boy Who Lived\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be in­volved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, al­though he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy any­where.\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but th\n"
     ]
    }
   ],
   "source": [
    "traing_path = '/Users/aaroncui/Desktop/UCL/NLP/NLP_project/data/hp1.txt'\n",
    "with open (traing_path, 'r') as f:\n",
    "    training_text = f.read()\n",
    "\n",
    "text_data = re.sub(r'\\n+', '\\n', training_text).strip()  # Remove excess newline characters\n",
    "print(text_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    # Define a function to load a dataset from a file using a specified tokenizer and optional block size.\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,  # Assign the tokenizer to be used for tokenizing the text data.\n",
    "        file_path = file_path,  # Specify the path to the file containing the text data.\n",
    "        block_size = block_size,  # Set the block size for splitting the text into chunks. Default is 128.\n",
    "    )\n",
    "\n",
    "    # Create an instance of TextDataset with the provided tokenizer, file path, and block size.\n",
    "    return dataset  # Return the created dataset object.\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    # Define a function named load_data_collator that takes two parameters: tokenizer and mlm (with a default value of False)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,   # Pass the tokenizer object to the DataCollatorForLanguageModeling constructor\n",
    "        mlm=mlm,             # Pass the mlm (Masked Language Modeling) flag to the DataCollatorForLanguageModeling constructor\n",
    "    )\n",
    "\n",
    "    # Create an instance of DataCollatorForLanguageModeling with the provided tokenizer and mlm flag\n",
    "    return data_collator  # Return the created data collator instance\n",
    "\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "    # Load the tokenizer from the specified model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    # Load the training dataset using the specified file path and tokenizer\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    # Load the data collator which is used to format the dataset for training\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    # Save the tokenizer to the specified output directory\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "    # Load the model from the specified model name\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Save the model to the specified output directory\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "          output_dir=output_dir,  # Directory to save the model and logs\n",
    "          overwrite_output_dir=overwrite_output_dir,  # Whether to overwrite the output directory\n",
    "          per_device_train_batch_size=per_device_train_batch_size,  # Batch size for training\n",
    "          num_train_epochs=num_train_epochs,  # Number of training epochs\n",
    "      )\n",
    "\n",
    "    # Initialize the Trainer with the model, training arguments, data collator, and training dataset\n",
    "    trainer = Trainer(\n",
    "          model=model,  # The model to train\n",
    "          args=training_args,  # Training arguments\n",
    "          data_collator=data_collator,  # Data collator for formatting the dataset\n",
    "          train_dataset=train_dataset,  # Training dataset\n",
    "  )\n",
    "      \n",
    "    # Start the training process\n",
    "    trainer.train()\n",
    "    # Save the trained model\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Not improved! Don't run this block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass\n",
    "# train_file_path = traing_path\n",
    "# model_name = 'gpt2'\n",
    "# output_dir = './models'\n",
    "# overwrite_output_dir = False\n",
    "# per_device_train_batch_size = 8\n",
    "# num_train_epochs = 50.0\n",
    "# save_steps = 50000\n",
    "\n",
    "\n",
    "\n",
    "# # Train\n",
    "# train(\n",
    "#     train_file_path=train_file_path,\n",
    "#     model_name=model_name,\n",
    "#     output_dir=output_dir,\n",
    "#     overwrite_output_dir=overwrite_output_dir,\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     num_train_epochs=num_train_epochs,\n",
    "#     save_steps=save_steps\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning with QA set - Start with small number of question answer pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "qa_data_path = '/Users/aaroncui/Desktop/UCL/NLP/NLP_project/data/Harry_Potter_Data_updated.json'\n",
    "\n",
    "with open(qa_data_path) as f:\n",
    "    qa_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(46)\n",
    "\n",
    "# 2. random sample 20 question answer pair\n",
    "question_number = torch.randperm(120)[:20].tolist()\n",
    "\n",
    "train_qa = {}\n",
    "train_qa['data'] = []\n",
    "for i in question_number:\n",
    "    tmp = {}\n",
    "    tmp['question'] = (\n",
    "    \"You are an assistant with expert knowledge of the Harry Potter series. \"\n",
    "    \"Based on the following passage, answer the question concisely in one sentence.\\n\\n\"\n",
    "    \"Passage:\\n\" + qa_data[i]['content'] + \"\\n\\n\"\n",
    "    \"Question: \" + qa_data[i]['question'] + \"\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "    tmp['answer'] = 'Correct option: ' + qa_data[i]['correct_answer_label'] + ' ' + qa_data[i]['correct_answer'] + '. ' + 'Reference text: ' + qa_data[i]['content']\n",
    "    train_qa['data'].append(tmp)\n",
    "\n",
    "output_file = '/Users/aaroncui/Desktop/UCL/NLP/NLP_project/data/Harry_Potter_qa_fine_tuning.json'\n",
    "\n",
    "with open(output_file, \"w\") as out_f:\n",
    "        json.dump(train_qa, out_f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n",
    "\n",
    "dataset = load_dataset('json', data_files='/content/drive/MyDrive/Colab Notebooks/NLP/Harry_Potter_qa_fine_tuning.json', field='data')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def format_and_tokenize(batch):\n",
    "    # Build a list of formatted strings from the batch data.\n",
    "    texts = [\n",
    "        f\"Question: {q}\\nAnswer: {a}\"\n",
    "        for q, a in zip(batch[\"question\"], batch[\"answer\"])\n",
    "    ]\n",
    "    print(texts)\n",
    "    # Tokenize the batch of texts\n",
    "    return tokenizer(texts, truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "tokenized_dataset = dataset.map(format_and_tokenize, batched=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Fine-Tune GPT-2\n",
    "# ---------------------------\n",
    "# Load the GPT-2 model with a language modeling head.\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Resize the model embeddings in case new tokens have been added.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set up training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/Colab Notebooks/NLP/models/custom_q_n_a2',\n",
    "    num_train_epochs=3,                    # Adjust the number of epochs as needed\n",
    "    per_device_train_batch_size=2,         # Adjust based on your GPU memory\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='no',              # Set to 'epoch' if you have a validation split\n",
    "    fp16=True,                              # Use mixed precision if supported\n",
    ")\n",
    "\n",
    "# Use the data collator to handle dynamic padding and prepare labels for language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Initialize the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],  # Adjust if your dataset is structured differently\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start fine-tuning.\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer.\n",
    "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/NLP/models/custom_q_n_a2\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/NLP/models/custom_q_n_a2\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Generate Answers with the Fine-Tuned Model\n",
    "# ---------------------------\n",
    "# Create a text generation pipeline using the saved model and tokenizer.\n",
    "text_generator = pipeline('text-generation', model='/content/drive/MyDrive/Colab Notebooks/NLP/models/custom_q_n_a2', tokenizer='/content/drive/MyDrive/Colab Notebooks/NLP/models/custom_q_n_a2')\n",
    "\n",
    "# Define a sample question prompt. The prompt should follow the same format used during training.\n",
    "sample_question = \"Who is the director of Grunnings?\"\n",
    "prompt = f\"Question: {sample_question}\\nAnswer:\"\n",
    "\n",
    "# Generate the answer.\n",
    "output = text_generator(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    #do_sample=True,         # Enable sampling for more diverse output.\n",
    "    #top_k=50,               # Consider the top 50 tokens at each step.\n",
    "    #top_p=0.90,             # Nucleus sampling: consider tokens with cumulative probability of 90%.\n",
    "    repetition_penalty=1.0, # Penalize repeated tokens.\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
