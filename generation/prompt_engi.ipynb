{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question ID: hp_001\n",
      "Prompt:\n",
      " Question: What curse caused the death of Harry Potter's parents?\n",
      "\n",
      "Options:\n",
      "A. Avada Kedavra\n",
      "B. Cruciatus Curse\n",
      "C. Imperius Curse\n",
      "D. Soul Extraction Curse\n",
      "\n",
      "Your Answer:\n",
      "Generated Answer: Q: Do you know where Harry Potter and Ron Weasley's bodies are?\n",
      "\n",
      "\n",
      "A\n",
      "Evaluation: Incorrect\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer and TensorFlow model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Example dataset (typically loaded from your JSON file)\n",
    "data = [\n",
    "    {\n",
    "        \"id\": \"hp_001\",\n",
    "        \"question\": \"What curse caused the death of Harry Potter's parents?\",\n",
    "        \"options\": [\n",
    "            \"Avada Kedavra\",\n",
    "            \"Cruciatus Curse\",\n",
    "            \"Imperius Curse\",\n",
    "            \"Soul Extraction Curse\"\n",
    "        ],\n",
    "        \"correct_answer\": \"Avada Kedavra\"\n",
    "    },\n",
    "    # Add more questions as needed\n",
    "]\n",
    "\n",
    "def create_prompt(question_obj):\n",
    "    \"\"\"\n",
    "    Create a prompt without including any additional content.\n",
    "    \"\"\"\n",
    "    question = question_obj['question']\n",
    "    options = question_obj['options']\n",
    "    \n",
    "    # Map options to letters (A, B, C, D, etc.)\n",
    "    option_map = {i: chr(65 + i) for i in range(len(options))}\n",
    "    options_text = \"\\n\".join([f\"{option_map[i]}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Options:\\n{options_text}\\n\\n\"\n",
    "        \"Your Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def evaluate_answer(generated_text, correct_answer):\n",
    "    \"\"\"\n",
    "    Evaluate whether the generated answer contains the correct answer.\n",
    "    This is a basic check; you can expand it for more robust evaluation.\n",
    "    \"\"\"\n",
    "    return correct_answer.lower() in generated_text.lower()\n",
    "\n",
    "results = []\n",
    "for question_obj in data:\n",
    "    prompt = create_prompt(question_obj)\n",
    "    # Encode the prompt using the tokenizer\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    \n",
    "    # Generate answer with GPT-2 (adjust max_length and other parameters as needed)\n",
    "    output_ids = model.generate(input_ids, max_length=input_ids.shape[1] + 20, do_sample=True)\n",
    "    \n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer portion after \"Your Answer:\"\n",
    "    answer_generated = generated_text.split(\"Your Answer:\")[-1].strip()\n",
    "    \n",
    "    # Evaluate if the answer is correct\n",
    "    is_correct = evaluate_answer(answer_generated, question_obj[\"correct_answer\"])\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": question_obj[\"id\"],\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_answer\": answer_generated,\n",
    "        \"evaluation\": \"Correct\" if is_correct else \"Incorrect\"\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "for res in results:\n",
    "    print(f\"Question ID: {res['id']}\")\n",
    "    print(\"Prompt:\\n\", res['prompt'])\n",
    "    print(\"Generated Answer:\", res['generated_answer'])\n",
    "    print(\"Evaluation:\", res['evaluation'])\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-gpt2-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
