{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/comp0197_tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Content: Each house in Hogwarts has a corresponding animal: Gryffindor (Lion), Ravenclaw (Eagle), Hufflepuff (Badger), and Slytherin (Snake), as explained in 'Harry Potter and the Philosopher’s Stone'.\n",
      "\n",
      "Question: What are the animal mascots of Gryffindor, Ravenclaw, Hufflepuff, and Slytherin, respectively?\n",
      "\n",
      "Options:\n",
      "A. Lion, Snake, Rat, Cow\n",
      "B. Lion, Eagle, Snake, Badger\n",
      "C. Sheep, Pig, Snake, Cow\n",
      "D. Lion, Hawk, Snake, Otter\n",
      "\n",
      "Please choose the correct option by outputting only one letter (A, B, C, or D) with no extra text.\n",
      "Your Answer: \n",
      "Generated Answer: A\n",
      "Evaluation: Incorrect (expected B)\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer and TensorFlow model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define the allowed letters and get their corresponding token IDs.\n",
    "allowed_letters = ['A', 'B', 'C', 'D']\n",
    "allowed_token_ids = [tokenizer.encode(letter, add_prefix_space=False)[0] for letter in allowed_letters]\n",
    "\n",
    "def create_prompt_with_content(question_obj):\n",
    "    \"\"\"\n",
    "    Create a prompt that uses the provided context, question, and options.\n",
    "    Instructs the model to output only one letter (A, B, C, or D) with no extra text.\n",
    "    \"\"\"\n",
    "    content = question_obj.get(\"content\", \"\")\n",
    "    options = question_obj['options']\n",
    "    \n",
    "    # Map options to letters (A, B, C, D, etc.)\n",
    "    option_map = {i: chr(65 + i) for i in range(len(options))}\n",
    "    options_text = \"\\n\".join([f\"{option_map[i]}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Content: {content}\\n\\n\"\n",
    "        f\"Question: {question_obj['question']}\\n\\n\"\n",
    "        f\"Options:\\n{options_text}\\n\\n\"\n",
    "        \"Please choose the correct option by outputting only one letter (A, B, C, or D) with no extra text.\\n\"\n",
    "        \"Your Answer: \"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_one_allowed_token(prompt, allowed_token_ids, temperature=0.7, do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate one token after the prompt, restricting selection to allowed_token_ids.\n",
    "    If do_sample=True, sample from the distribution using the provided temperature.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    \n",
    "    # Run the model to get logits with return_dict=True\n",
    "    outputs = model(input_ids, return_dict=True)\n",
    "    logits = outputs.logits  # shape: (batch_size, seq_length, vocab_size)\n",
    "    \n",
    "    # Get logits for the last token (the next-token logits)\n",
    "    last_token_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "    last_token_logits_np = last_token_logits.numpy()  # convert to numpy array\n",
    "    \n",
    "    # Create a masked logits vector: set non-allowed tokens to a very low score.\n",
    "    masked_logits = np.full(last_token_logits_np.shape, -1e9)\n",
    "    for token_id in allowed_token_ids:\n",
    "        masked_logits[0, token_id] = last_token_logits_np[0, token_id]\n",
    "    \n",
    "    if do_sample:\n",
    "        # Apply temperature scaling\n",
    "        scaled_logits = masked_logits / temperature\n",
    "        \n",
    "        # Compute probabilities using softmax\n",
    "        exp_logits = np.exp(scaled_logits)\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        \n",
    "        # Sample one token from allowed tokens\n",
    "        next_token_id = int(np.random.choice(len(probs[0]), p=probs[0]))\n",
    "    else:\n",
    "        # Deterministic: take the highest probability token\n",
    "        next_token_id = int(np.argmax(masked_logits))\n",
    "        \n",
    "    return next_token_id\n",
    "\n",
    "def evaluate_answer(question_obj, generated_letter):\n",
    "    \"\"\"\n",
    "    Evaluate whether the generated letter corresponds to the correct answer.\n",
    "    Maps the correct answer text to its corresponding letter and compares.\n",
    "    \"\"\"\n",
    "    correct_answer_text = question_obj[\"correct_answer\"]\n",
    "    options = question_obj[\"options\"]\n",
    "    try:\n",
    "        correct_index = options.index(correct_answer_text)\n",
    "        correct_letter = chr(65 + correct_index)\n",
    "    except ValueError:\n",
    "        return False, None\n",
    "    \n",
    "    is_correct = (generated_letter == correct_letter)\n",
    "    return is_correct, correct_letter\n",
    "\n",
    "# Example question object\n",
    "question_obj = {\n",
    "    \"id\": \"hp_004\",\n",
    "    \"question\": \"What are the animal mascots of Gryffindor, Ravenclaw, Hufflepuff, and Slytherin, respectively?\",\n",
    "    \"options\": [\n",
    "      \"Lion, Snake, Rat, Cow\",\n",
    "      \"Lion, Eagle, Snake, Badger\",\n",
    "      \"Sheep, Pig, Snake, Cow\",\n",
    "      \"Lion, Hawk, Snake, Otter\"\n",
    "    ],\n",
    "    \"correct_answer\": \"Lion, Eagle, Snake, Badger\",\n",
    "    \"content\": \"Each house in Hogwarts has a corresponding animal: Gryffindor (Lion), Ravenclaw (Eagle), Hufflepuff (Badger), and Slytherin (Snake), as explained in 'Harry Potter and the Philosopher’s Stone'.\"\n",
    "  }\n",
    "\n",
    "# Create the prompt (with content)\n",
    "prompt = create_prompt_with_content(question_obj)\n",
    "\n",
    "# Generate one token restricted to the allowed letters\n",
    "next_token_id = generate_one_allowed_token(prompt, allowed_token_ids)\n",
    "answer_generated = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "# Evaluate the answer\n",
    "is_correct, correct_letter = evaluate_answer(question_obj, answer_generated)\n",
    "\n",
    "print(\"Prompt:\\n\", prompt)\n",
    "print(\"Generated Answer:\", answer_generated)\n",
    "print(\"Evaluation:\", \"Correct\" if is_correct else f\"Incorrect (expected {correct_letter})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aaroncui/Desktop/UCL/NLP/NLP_project/generation/data/Harry_Potter_Data_updated.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/aaroncui/Desktop/UCL/NLP/NLP_project/generation/data/Harry_Potter_Data_updated.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m file_path = os.path.abspath(\u001b[33m\"\u001b[39m\u001b[33mdata/Harry_Potter_Data_updated.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(file_path)  \u001b[38;5;66;03m# Print to confirm it's correct\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     data = json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/comp0197_tf/lib/python3.11/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/aaroncui/Desktop/UCL/NLP/NLP_project/generation/data/Harry_Potter_Data_updated.json'"
     ]
    }
   ],
   "source": [
    "file_path = os.path.abspath(\"data/Harry_Potter_Data_updated.json\")\n",
    "print(file_path)  # Print to confirm it's correct\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-gpt2-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
