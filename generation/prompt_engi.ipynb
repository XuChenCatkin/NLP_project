{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer and TensorFlow model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Define the allowed letters and get their corresponding token IDs.\n",
    "allowed_letters = ['A', 'B', 'C', 'D']\n",
    "allowed_token_ids = [tokenizer.encode(letter, add_prefix_space=False)[0] for letter in allowed_letters]\n",
    "\n",
    "def create_prompt_with_content(question_obj):\n",
    "    \"\"\"\n",
    "    Create a prompt that uses the provided context, question, and options.\n",
    "    Instructs the model to output only one letter (A, B, C, or D) with no extra text.\n",
    "    \"\"\"\n",
    "    content = question_obj.get(\"content\", \"\")\n",
    "    options = question_obj['options']\n",
    "    \n",
    "    # Map options to letters (A, B, C, D, etc.)\n",
    "    option_map = {i: chr(65 + i) for i in range(len(options))}\n",
    "    options_text = \"\\n\".join([f\"{option_map[i]}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    prompt = (\n",
    "        \n",
    "        f\"Content: {content}\\n\\n\"\n",
    "        f\"Question: {question_obj['question']}\\n\\n\"\n",
    "        f\"Options:\\n{options_text}\\n\\n\"\n",
    "        \"Please choose the correct option by outputting only one letter (A, B, C, or D) with no extra text.\\n\"\n",
    "        \"Your Answer: \"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_one_allowed_token(prompt, allowed_token_ids, temperature=1, do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate one token after the prompt, restricting selection to allowed_token_ids.\n",
    "    If do_sample=True, sample from the distribution using the provided temperature.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    \n",
    "    # Run the model to get logits with return_dict=True\n",
    "    outputs = model(input_ids, return_dict=True)\n",
    "    logits = outputs.logits  # shape: (batch_size, seq_length, vocab_size)\n",
    "    \n",
    "    # Get logits for the last token (the next-token logits)\n",
    "    last_token_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "    last_token_logits_np = last_token_logits.numpy()  # convert to numpy array\n",
    "    \n",
    "    # Create a masked logits vector: set non-allowed tokens to a very low score.\n",
    "    masked_logits = np.full(last_token_logits_np.shape, -1e9)\n",
    "    for token_id in allowed_token_ids:\n",
    "        masked_logits[0, token_id] = last_token_logits_np[0, token_id]\n",
    "    \n",
    "    if do_sample:\n",
    "        # Apply temperature scaling\n",
    "        scaled_logits = masked_logits / temperature\n",
    "        \n",
    "        # Compute probabilities using softmax\n",
    "        exp_logits = np.exp(scaled_logits)\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        \n",
    "        # Sample one token from allowed tokens\n",
    "        next_token_id = int(np.random.choice(len(probs[0]), p=probs[0]))\n",
    "    else:\n",
    "        # Deterministic: take the highest probability token\n",
    "        next_token_id = int(np.argmax(masked_logits))\n",
    "        \n",
    "    return next_token_id\n",
    "\n",
    "def evaluate_answer(question_obj, generated_letter):\n",
    "    \"\"\"\n",
    "    Evaluate whether the generated letter corresponds to the correct answer.\n",
    "    Maps the correct answer text to its corresponding letter and compares.\n",
    "    \"\"\"\n",
    "    correct_answer_text = question_obj[\"correct_answer\"]\n",
    "    options = question_obj[\"options\"]\n",
    "    try:\n",
    "        correct_index = options.index(correct_answer_text)\n",
    "        correct_letter = chr(65 + correct_index)\n",
    "    except ValueError:\n",
    "        return False, None\n",
    "    \n",
    "    is_correct = (generated_letter == correct_letter)\n",
    "    return is_correct, correct_letter\n",
    "\n",
    "# # Example question object\n",
    "# question_obj = {\n",
    "#     \"id\": \"hp_004\",\n",
    "#     \"question\": \"What are the animal mascots of Gryffindor, Ravenclaw, Hufflepuff, and Slytherin, respectively?\",\n",
    "#     \"options\": [\n",
    "#       \"Lion, Snake, Rat, Cow\",\n",
    "#       \"Lion, Eagle, Snake, Badger\",\n",
    "#       \"Sheep, Pig, Snake, Cow\",\n",
    "#       \"Lion, Hawk, Snake, Otter\"\n",
    "#     ],\n",
    "#     \"correct_answer\": \"Lion, Eagle, Snake, Badger\",\n",
    "#     \"content\": \"Each house in Hogwarts has a corresponding animal: Gryffindor (Lion), Ravenclaw (Eagle), Hufflepuff (Badger), and Slytherin (Snake), as explained in 'Harry Potter and the Philosopherâ€™s Stone'.\"\n",
    "#   }\n",
    "\n",
    "# # Create the prompt (with content)\n",
    "# prompt = create_prompt_with_content(question_obj)\n",
    "\n",
    "# # Generate one token restricted to the allowed letters\n",
    "# next_token_id = generate_one_allowed_token(prompt, allowed_token_ids)\n",
    "# answer_generated = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "# # Evaluate the answer\n",
    "# is_correct, correct_letter = evaluate_answer(question_obj, answer_generated)\n",
    "\n",
    "# print(\"Prompt:\\n\", prompt)\n",
    "# print(\"Generated Answer:\", answer_generated)\n",
    "# print(\"Evaluation:\", \"Correct\" if is_correct else f\"Incorrect (expected {correct_letter})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this file_path\n",
    "# D:\\NLP_Project\\NLP_project\\data\\Harry_Potter_Data_updated.json\n",
    "# D:\\NLP_Project\\NLP_project\\data\\Harry_Potter_Data_updated_shuffled.json\n",
    "file_path = os.path.abspath(r\"D:\\NLP_Project\\NLP_project\\data\\Harry_Potter_Data_updated_shuffled.json\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 70\n",
      "B: 9\n",
      "C: 13\n",
      "D: 28\n",
      "correct_count: 13\n",
      "count: 120\n",
      "accuracy: 0.10833333333333334\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "correct_count = 0 \n",
    "generate_answers = []\n",
    "for item in data:\n",
    "    question_obj = item\n",
    "\n",
    "    # Create the prompt (with content)\n",
    "    prompt = create_prompt_with_content(question_obj)\n",
    "\n",
    "    # Generate one token restricted to the allowed letters\n",
    "    next_token_id = generate_one_allowed_token(prompt, allowed_token_ids)\n",
    "    answer_generated = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "    # Evaluate the answer\n",
    "    is_correct, correct_letter = evaluate_answer(question_obj, answer_generated)\n",
    "    count += 1\n",
    "\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    \n",
    "    # record the generated answer\n",
    "    \n",
    "    generate_answers.append(answer_generated)\n",
    "    # print(\"Prompt:\\n\", prompt)\n",
    "    # print(\"Generated Answer:\", answer_generated)\n",
    "    # print(\"Evaluation:\", \"Correct\" if is_correct else f\"Incorrect (expected {correct_letter})\")\n",
    "# print num of A B C D \n",
    "print(f'A: {generate_answers.count(\"A\")}')\n",
    "print(f'B: {generate_answers.count(\"B\")}')\n",
    "print(f'C: {generate_answers.count(\"C\")}')\n",
    "print(f'D: {generate_answers.count(\"D\")}')\n",
    "print(f'correct_count: {correct_count}')\n",
    "print(f'count: {count}')\n",
    "print(f'accuracy: {correct_count/count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Prompt:\n",
      " Example 1:\n",
      "Content: In 'Harry Potter and the Chamber of Secrets', it is revealed that a mysterious basilisk terrorizes the school.\n",
      "Question: What creature is terrorizing Hogwarts in this example?\n",
      "Options:\n",
      "A. Dragon\n",
      "B. Basilisk\n",
      "C. Troll\n",
      "D. Unicorn\n",
      "Your Answer: B\n",
      "\n",
      "Example 2:\n",
      "Content: In 'Harry Potter and the Prisoner of Azkaban', it is shown that Remus Lupin is a werewolf.\n",
      "Question: What is Remus Lupin's secret condition?\n",
      "Options:\n",
      "A. Vampire\n",
      "B. Werewolf\n",
      "C. Ghost\n",
      "D. Muggle\n",
      "Your Answer: B\n",
      "\n",
      "Content: In 'Harry Potter and the Philosopher's Stone', it is revealed that Lord Voldemort killed James and Lily Potter using the Avada Kedavra curse. This is one of the three Unforgivable Curses and causes instant death without physical harm.\n",
      "\n",
      "Question: What curse caused the death of Harry Potter's parents?\n",
      "\n",
      "Options:\n",
      "A. Cruciatus Curse\n",
      "B. Imperius Curse\n",
      "C. Avada Kedavra\n",
      "D. Soul Extraction Curse\n",
      "\n",
      "Please choose the correct option by outputting only one letter (A, B, C, or D) with no extra text.\n",
      "Your Answer: \n",
      "Generated Answer: A\n",
      "Evaluation: Incorrect (expected C)\n"
     ]
    }
   ],
   "source": [
    "# Few-shot examples to include in the prompt.\n",
    "few_shot_examples = (\n",
    "    \"Example 1:\\n\"\n",
    "    \"Content: In 'Harry Potter and the Chamber of Secrets', it is revealed that a mysterious basilisk terrorizes the school.\\n\"\n",
    "    \"Question: What creature is terrorizing Hogwarts in this example?\\n\"\n",
    "    \"Options:\\n\"\n",
    "    \"A. Dragon\\n\"\n",
    "    \"B. Basilisk\\n\"\n",
    "    \"C. Troll\\n\"\n",
    "    \"D. Unicorn\\n\"\n",
    "    \"Your Answer: B\\n\\n\"\n",
    "    \"Example 2:\\n\"\n",
    "    \"Content: In 'Harry Potter and the Prisoner of Azkaban', it is shown that Remus Lupin is a werewolf.\\n\"\n",
    "    \"Question: What is Remus Lupin's secret condition?\\n\"\n",
    "    \"Options:\\n\"\n",
    "    \"A. Vampire\\n\"\n",
    "    \"B. Werewolf\\n\"\n",
    "    \"C. Ghost\\n\"\n",
    "    \"D. Muggle\\n\"\n",
    "    \"Your Answer: B\\n\\n\"\n",
    ")\n",
    "\n",
    "def create_few_shot_prompt_with_content(question_obj, few_shot_text=few_shot_examples):\n",
    "    \"\"\"\n",
    "    Create a prompt that includes a few-shot demonstration of how to answer,\n",
    "    followed by the current question with its content, question, and options.\n",
    "    Instructs the model to output only one letter (A, B, C, or D) with no extra text.\n",
    "    \"\"\"\n",
    "    content = question_obj.get(\"content\", \"\")\n",
    "    options = question_obj['options']\n",
    "    \n",
    "    # Map options to letters (A, B, C, D, etc.)\n",
    "    option_map = {i: chr(65 + i) for i in range(len(options))}\n",
    "    options_text = \"\\n\".join([f\"{option_map[i]}. {option}\" for i, option in enumerate(options)])\n",
    "    \n",
    "    test_prompt = (\n",
    "        f\"Content: {content}\\n\\n\"\n",
    "        f\"Question: {question_obj['question']}\\n\\n\"\n",
    "        f\"Options:\\n{options_text}\\n\\n\"\n",
    "        \"Please choose the correct option by outputting only one letter (A, B, C, or D) with no extra text.\\n\"\n",
    "        \"Your Answer: \"\n",
    "    )\n",
    "    \n",
    "    # Combine the few-shot examples with the test question.\n",
    "    full_prompt = few_shot_text + test_prompt\n",
    "    return full_prompt\n",
    "\n",
    "# The rest of your code for generation and evaluation remains the same.\n",
    "def generate_one_allowed_token(prompt, allowed_token_ids, temperature=1, do_sample=True):\n",
    "    \"\"\"\n",
    "    Generate one token after the prompt, restricting selection to allowed_token_ids.\n",
    "    If do_sample=True, sample from the distribution using the provided temperature.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
    "    \n",
    "    # Run the model to get logits with return_dict=True\n",
    "    outputs = model(input_ids, return_dict=True)\n",
    "    logits = outputs.logits  # shape: (batch_size, seq_length, vocab_size)\n",
    "    \n",
    "    # Get logits for the last token (the next-token logits)\n",
    "    last_token_logits = logits[:, -1, :]  # shape: (1, vocab_size)\n",
    "    last_token_logits_np = last_token_logits.numpy()  # convert to numpy array\n",
    "    \n",
    "    # Create a masked logits vector: set non-allowed tokens to a very low score.\n",
    "    masked_logits = np.full(last_token_logits_np.shape, -1e9)\n",
    "    for token_id in allowed_token_ids:\n",
    "        masked_logits[0, token_id] = last_token_logits_np[0, token_id]\n",
    "    \n",
    "    if do_sample:\n",
    "        # Apply temperature scaling\n",
    "        scaled_logits = masked_logits / temperature\n",
    "        \n",
    "        # Compute probabilities using softmax\n",
    "        exp_logits = np.exp(scaled_logits)\n",
    "        probs = exp_logits / np.sum(exp_logits)\n",
    "        \n",
    "        # Sample one token from allowed tokens\n",
    "        next_token_id = int(np.random.choice(len(probs[0]), p=probs[0]))\n",
    "    else:\n",
    "        # Deterministic: take the highest probability token\n",
    "        next_token_id = int(np.argmax(masked_logits))\n",
    "        \n",
    "    return next_token_id\n",
    "\n",
    "def evaluate_answer(question_obj, generated_letter):\n",
    "    \"\"\"\n",
    "    Evaluate whether the generated letter corresponds to the correct answer.\n",
    "    Maps the correct answer text to its corresponding letter and compares.\n",
    "    \"\"\"\n",
    "    correct_answer_text = question_obj[\"correct_answer\"]\n",
    "    options = question_obj[\"options\"]\n",
    "    try:\n",
    "        correct_index = options.index(correct_answer_text)\n",
    "        correct_letter = chr(65 + correct_index)\n",
    "    except ValueError:\n",
    "        return False, None\n",
    "    \n",
    "    is_correct = (generated_letter == correct_letter)\n",
    "    return is_correct, correct_letter\n",
    "\n",
    "# Example question object (test question)\n",
    "question_obj = {\n",
    "    \"id\": \"hp_001\",\n",
    "    \"question\": \"What curse caused the death of Harry Potter's parents?\",\n",
    "    \"options\": [\n",
    "        \"Cruciatus Curse\",\n",
    "        \"Imperius Curse\",\n",
    "        \"Avada Kedavra\",\n",
    "        \"Soul Extraction Curse\"\n",
    "    ],\n",
    "    \"correct_answer\": \"Avada Kedavra\",\n",
    "    \"content\": \"In 'Harry Potter and the Philosopher's Stone', it is revealed that Lord Voldemort killed James and Lily Potter using the Avada Kedavra curse. This is one of the three Unforgivable Curses and causes instant death without physical harm.\"\n",
    "}\n",
    "\n",
    "# Create the few-shot prompt (with content)\n",
    "prompt = create_few_shot_prompt_with_content(question_obj)\n",
    "\n",
    "# Generate one token restricted to the allowed letters\n",
    "next_token_id = generate_one_allowed_token(prompt, allowed_token_ids, temperature=1, do_sample=True)\n",
    "answer_generated = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "# Evaluate the answer\n",
    "is_correct, correct_letter = evaluate_answer(question_obj, answer_generated)\n",
    "\n",
    "print(\"Full Prompt:\\n\", prompt)\n",
    "print(\"Generated Answer:\", answer_generated)\n",
    "print(\"Evaluation:\", \"Correct\" if is_correct else f\"Incorrect (expected {correct_letter})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for chunk 1: (1, 50257)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# # Load pre-trained GPT-2 tokenizer and TensorFlow model\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# model = TFGPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "\n",
    "# model.trainable = False  # We don't need to fine-tune here\n",
    "# Load model with LM head\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "model.trainable = False  # For inference\n",
    "# Define a function to compute the embedding for a given text passage\n",
    "def embed_text(text):\n",
    "    # Tokenize the input text and convert to TensorFlow tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "    # Forward pass through GPT-2 to get hidden states (returns a tuple)\n",
    "    outputs = model(inputs)\n",
    "    # Get the last hidden state (the first element of the tuple)\n",
    "    last_hidden_state = outputs[0]\n",
    "    # Average the token embeddings to form a single sentence embedding\n",
    "    sentence_embedding = tf.reduce_mean(last_hidden_state, axis=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "# Example dataset: a list of chunk dictionaries\n",
    "data = [\n",
    "    {\n",
    "        \"title_num\": 1,\n",
    "        \"title\": \"Harry Potter and the Philosopher's Stone\",\n",
    "        \"chapter_num\": 1,\n",
    "        \"chapter_name\": \"The Boy Who Lived\",\n",
    "        \"passage\": (\n",
    "            \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \"\n",
    "            \"that they were perfectly normal, thank you very much. They were the last \"\n",
    "            \"people you'd expect to be involved in anything strange or mysterious, because \"\n",
    "            \"they just didn't hold with such nonsense. Mr. Dursley was the director of a firm \"\n",
    "            \"called Grunnings, which made drills. He was a big, beefy man with hardly any neck, \"\n",
    "            \"although he did have a very large mustache. Mrs. Dursley was thin and blonde and had \"\n",
    "            \"nearly twice the usual amount of neck, which came in very useful as she spent so much \"\n",
    "            \"of her time craning over garden fences, spying on the neighbors.\"\n",
    "        )\n",
    "    }\n",
    "    # Add additional chunks as needed...\n",
    "]\n",
    "\n",
    "# Process each chunk and compute its embedding\n",
    "embeddings = []\n",
    "for chunk in data:\n",
    "    passage = chunk[\"passage\"]\n",
    "    embedding = embed_text(passage)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Print the shape of each embedding to verify\n",
    "for i, emb in enumerate(embeddings):\n",
    "    print(f\"Embedding for chunk {i+1}: {emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: You are an assistant with expert knowledge of the Harry Potter series. Based on the following passage, answer the question concisely in one sentence.\n",
      "\n",
      "Passage:\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.\n",
      "\n",
      "Question: What is Mr. Dursleyâ€™s role at Grunnings?\n",
      "Answer:\n",
      "\n",
      "He is the head of the firm, and he is responsible for all of the work that goes on at the firm. He is also responsible for the production of Harry Potter and the Philosopher's Stone, which is produced by Grunn\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load model with LM head (already including output_hidden_states=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "model.trainable = False  # For inference\n",
    "\n",
    "# Define the embedding function (using average pooling over last hidden state)\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)  # outputs is a tuple; use outputs[0] for last hidden state\n",
    "    last_hidden_state = outputs[0]\n",
    "    sentence_embedding = tf.reduce_mean(last_hidden_state, axis=1)\n",
    "    return sentence_embedding  # shape: (1, hidden_dim)\n",
    "\n",
    "# Example dataset: a list of document chunks\n",
    "data = [\n",
    "    {\n",
    "        \"title_num\": 1,\n",
    "        \"title\": \"Harry Potter and the Philosopher's Stone\",\n",
    "        \"chapter_num\": 1,\n",
    "        \"chapter_name\": \"The Boy Who Lived\",\n",
    "        \"passage\": (\n",
    "            \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say \"\n",
    "            \"that they were perfectly normal, thank you very much. They were the last \"\n",
    "            \"people you'd expect to be involved in anything strange or mysterious, because \"\n",
    "            \"they just didn't hold with such nonsense. Mr. Dursley was the director of a firm \"\n",
    "            \"called Grunnings, which made drills. He was a big, beefy man with hardly any neck, \"\n",
    "            \"although he did have a very large mustache. Mrs. Dursley was thin and blonde and had \"\n",
    "            \"nearly twice the usual amount of neck, which came in very useful as she spent so much \"\n",
    "            \"of her time craning over garden fences, spying on the neighbors.\"\n",
    "        )\n",
    "    }\n",
    "    # You can add more chunks if needed\n",
    "]\n",
    "\n",
    "# Precompute and store embeddings for all passages\n",
    "embeddings = []\n",
    "for chunk in data:\n",
    "    passage = chunk[\"passage\"]\n",
    "    emb = embed_text(passage)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Define a function to compute cosine similarity between two embeddings\n",
    "def cosine_similarity(a, b):\n",
    "    a_norm = tf.nn.l2_normalize(a, axis=1)\n",
    "    b_norm = tf.nn.l2_normalize(b, axis=1)\n",
    "    return tf.reduce_sum(a_norm * b_norm, axis=1)  # returns a tensor with similarity score\n",
    "\n",
    "# --- Retrieval and QA Pipeline ---\n",
    "\n",
    "# 1. Define your question\n",
    "question = \"What is Mr. Dursleyâ€™s role at Grunnings?\"\n",
    "\n",
    "# 2. Compute the embedding for the question\n",
    "question_emb = embed_text(question)  # shape: (1, hidden_dim)\n",
    "\n",
    "# 3. Compare with stored passage embeddings to find the most relevant passage\n",
    "similarities = []\n",
    "for emb in embeddings:\n",
    "    sim = cosine_similarity(question_emb, emb)\n",
    "    similarities.append(sim.numpy()[0])  # Extract scalar similarity\n",
    "\n",
    "best_idx = np.argmax(similarities)\n",
    "retrieved_passage = data[best_idx][\"passage\"]\n",
    "\n",
    "# 4. Construct a prompt that includes the retrieved passage\n",
    "prompt = (\n",
    "    \"You are an assistant with expert knowledge of the Harry Potter series. \"\n",
    "    \"Based on the following passage, answer the question concisely in one sentence.\\n\\n\"\n",
    "    \"Passage:\\n\" + retrieved_passage + \"\\n\\n\"\n",
    "    \"Question: \" + question + \"\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# 5. Tokenize and generate the answer\n",
    "inputs = tokenizer(prompt, return_tensors=\"tf\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=250,\n",
    "    pad_token_id=50256,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=3,\n",
    "    early_stopping=True\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Answer:\", generated_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
