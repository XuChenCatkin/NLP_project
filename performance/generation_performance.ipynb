{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from evaluation.eval_utils import *\n",
    "from evaluation.eval_utils import *\n",
    "import json\n",
    "#from retrieval import *\n",
    "import numpy as np\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the generation model here\n",
    "from generation.cohere_generation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9251 corpus passages\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Retrieval Method Functions\n",
    "#########################################\n",
    "\n",
    "def tfidf_retrieval(query, config):\n",
    "    \"\"\"\n",
    "    TF-IDF retrieval.\n",
    "    config must include:\n",
    "      - \"vectorizer\": a fitted TfidfVectorizer,\n",
    "      - \"doc_matrix\": document-term matrix,\n",
    "      - \"passages\": list of passages,\n",
    "      - \"chunk_ids\": list of passage identifiers,\n",
    "      - \"top_k\": number of top results.\n",
    "    \"\"\"\n",
    "    vectorizer = config[\"vectorizer\"]\n",
    "    doc_matrix = config[\"doc_matrix\"]\n",
    "    passages = config[\"passages\"]\n",
    "    chunk_ids = config[\"chunk_ids\"]\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    \n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_similarities = (doc_matrix @ query_vec.T).toarray().flatten()\n",
    "    sorted_indices = np.argsort(cosine_similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = [{\"chunk_id\": chunk_ids[i], \"score\": float(cosine_similarities[i])} \n",
    "               for i in sorted_indices]\n",
    "    return results\n",
    "\n",
    "def bm25_retrieval(query, config):\n",
    "    \"\"\"\n",
    "    BM25 retrieval.\n",
    "    config must include:\n",
    "      - \"bm25\": a BM25Okapi object,\n",
    "      - \"passages\": list of passages,\n",
    "      - \"chunk_ids\": list of passage identifiers,\n",
    "      - \"top_k\": number of top results.\n",
    "    \"\"\"\n",
    "    bm25 = config[\"bm25\"]\n",
    "    passages = config[\"passages\"]\n",
    "    chunk_ids = config[\"chunk_ids\"]\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    \n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    results = [{\"chunk_id\": chunk_ids[i], \"score\": float(scores[i])} \n",
    "               for i in sorted_indices]\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def dense_retrieval_subqueries(queries, config):\n",
    "\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "\n",
    "    results = []\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    for query in queries:\n",
    "        query_emb = query_embed_search(query, config[\"all_subqueries\"], config[\"subquery_index\"])\n",
    "        query_emb = query_emb.reshape(1, -1)  # Reshape to (1, d)\n",
    "        distances, indices = config[\"faiss_index\"].search(query_emb, top_k)\n",
    "        results.extend([\n",
    "            {\n",
    "                \"sub_query\": query,\n",
    "                \"chunk_id\": config[\"chunk_ids\"][i],\n",
    "                \"passage\": config[\"passages\"][i],\n",
    "                \"score\": float(distances[0][j])\n",
    "            } for j, i in enumerate(indices[0])\n",
    "        ])\n",
    "    return results\n",
    "\n",
    "def query_embed_search(query, all_queries_list, index):\n",
    "    \"\"\"\n",
    "    Given a query, finds its embedding from the precomputed subqueries index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        position = all_queries_list.index(query)\n",
    "        return index.reconstruct(position)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Query '{query}' not found in the list of all queries.\")\n",
    "    \n",
    "\n",
    "def hybrid(query, config):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval: first retrieve candidates with one method, then re-rank with the other.\n",
    "    \n",
    "    The order is determined by config[\"order\"]:\n",
    "      - \"sparse_dense\" (default): Use query[0] (or query if not a list) for sparse retrieval,\n",
    "                                   then query[1] (or query) for dense retrieval.\n",
    "      - \"dense_sparse\": Use query[0] for dense retrieval,\n",
    "                        then query[1] for sparse retrieval.\n",
    "    \n",
    "    The candidate sets are combined (using the \"mode\": \"intersection\" or \"union\")\n",
    "    and then sorted by the score of the re-ranking model.\n",
    "    \n",
    "    Parameters:\n",
    "      - query (str or list): If a list of at least two elements, the two parts will be used\n",
    "                             in the order specified by config[\"order\"]. Otherwise, the same query is used.\n",
    "      - config (dict): Must contain:\n",
    "          * \"order\": either \"sparse_dense\" (default) or \"dense_sparse\"\n",
    "          * \"sparse_func\": sparse retrieval function (e.g., tfidf_retrieval or bm25_retrieval)\n",
    "          * \"sparse_config\": configuration for the sparse method.\n",
    "          * \"dense_func\": dense retrieval function (e.g., dense_retrieval_subqueries)\n",
    "          * \"dense_config\": configuration for the dense method.\n",
    "          * \"intermediate_k\": candidate set size (default: 30)\n",
    "          * \"final_k\": final result count (default: 5)\n",
    "          * \"mode\": \"intersection\" (default) or \"union\"\n",
    "    \n",
    "    Returns:\n",
    "      A list of final results (dictionaries) from the re-ranking step.\n",
    "    \"\"\"\n",
    "    order = config.get(\"order\", \"sparse_dense\")\n",
    "    intermediate_k = config.get(\"intermediate_k\", 30)\n",
    "    final_k = config.get(\"final_k\", 5)\n",
    "    mode = config.get(\"mode\", \"intersection\")\n",
    "    \n",
    "    # Decide which query part to use for each retrieval step.\n",
    "    if isinstance(query, list) and len(query) >= 2:\n",
    "        query_sparse = query[0]\n",
    "        query_dense = query[1]\n",
    "    else:\n",
    "        print(\"Query should be a list of two queries: [sparse_query, dense_query].\")\n",
    "\n",
    "\n",
    "    if order == \"sparse_dense\":\n",
    "        # Sparse retrieval first.\n",
    "        sparse_config = config[\"sparse_config\"].copy()\n",
    "        sparse_config[\"top_k\"] = intermediate_k\n",
    "        sparse_results = config[\"sparse_func\"](query_sparse, sparse_config)\n",
    "        \n",
    "        # Dense retrieval.\n",
    "        dense_results = config[\"dense_func\"](query_dense, config[\"dense_config\"])\n",
    "        \n",
    "        # Combine: here we choose to filter dense results by the candidate set from sparse.\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in sparse_results)\n",
    "        filtered_dense = [r for r in dense_results if r[\"chunk_id\"] in candidate_ids]\n",
    "        if not filtered_dense:\n",
    "            filtered_dense = dense_results\n",
    "        # Sort by dense score.\n",
    "        sorted_results = sorted(filtered_dense, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    elif order == \"dense_sparse\":\n",
    "        # Dense retrieval first.\n",
    "        dense_results = config[\"dense_func\"](query_dense, config[\"dense_config\"])\n",
    "        # Sparse retrieval.\n",
    "        sparse_config = config[\"sparse_config\"].copy()\n",
    "        sparse_config[\"top_k\"] = intermediate_k\n",
    "        sparse_results = config[\"sparse_func\"](query_sparse, sparse_config)\n",
    "        # Combine: filter sparse results by dense candidate set.\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in dense_results)\n",
    "        filtered_sparse = [r for r in sparse_results if r[\"chunk_id\"] in candidate_ids]\n",
    "        if not filtered_sparse:\n",
    "            filtered_sparse = sparse_results\n",
    "        # Sort by sparse score.\n",
    "        sorted_results = sorted(filtered_sparse, key=lambda x: x[\"score\"], reverse=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid order specified.\")\n",
    "    \n",
    "    return sorted_results[:final_k]\n",
    "\n",
    "#########################################\n",
    "# Dataset Configuration\n",
    "#########################################\n",
    "\n",
    "# Global corpus (for all retrieval methods that use it).\n",
    "def load_corpus(corpus_file):\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    passages = [entry[\"passage\"] for entry in data if \"passage\" in entry]\n",
    "    chunk_ids = [entry[\"chunk_id\"] for entry in data if \"chunk_id\" in entry]\n",
    "    print(f\"Loaded {len(passages)} corpus passages\")\n",
    "    return passages, chunk_ids\n",
    "\n",
    "# A helper function to load subqueries from a ground truth file.\n",
    "def retrieve_all_subqueries(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "    subqueries = []\n",
    "    for item in qa_data:\n",
    "        subqueries.extend(item[\"sub_questions\"])\n",
    "    return subqueries\n",
    "\n",
    "# Paths for the corpus and QA sets.\n",
    "CORPUS_FILE = \"../data/chunked_text_all_together_cleaned.json\"\n",
    "QA_PATH = \"../data/QA_set\"\n",
    "QA_EMBEDDED_PATH = \"../data/QA_set_embedded\"\n",
    "PM_PATH = \"../performance\"\n",
    "\n",
    "# CORPUS_FILE = os.path.join(project_root, \"data\", \"chunked_text_all_together_cleaned.json\")\n",
    "# QA_PATH = os.path.join(project_root, \"data\", \"QA_set\")\n",
    "# QA_EMBEDDED_PATH = os.path.join(project_root, \"data\", \"QA_set_embedded\")\n",
    "# Load the corpus and build common indexes.\n",
    "\n",
    "passages, chunk_ids = load_corpus(CORPUS_FILE)\n",
    "\n",
    "# Build TF-IDF index for the corpus.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_matrix = tfidf_vectorizer.fit_transform(passages)\n",
    "\n",
    "# Build BM25 index for the corpus.\n",
    "tokenized_passages = [word_tokenize(p.lower()) for p in passages]\n",
    "bm25 = BM25Okapi(tokenized_passages)\n",
    "\n",
    "# Load global dense corpus index (FAISS).\n",
    "corpus_dense_index = faiss.read_index(\"../hp_all_bge.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"easy_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"easy_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"easy_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_easy_single_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"medium_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"medium_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"medium_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_medium_single_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"medium_multi\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"medium_multi_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"medium_multi_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_medium_multi_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"hard_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"hard_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"hard_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_hard_single_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"hard_multi\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"hard_multi_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"hard_multi_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_hard_multi_labeled.index\"))\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dictionary mapping retrieval method names to their functions.\n",
    "retrieval_methods = {\n",
    "    \"tfidf\": tfidf_retrieval,\n",
    "    \"bm25\": bm25_retrieval,\n",
    "    \"dense\": dense_retrieval_subqueries,\n",
    "    \"tfidf_dense\": hybrid,\n",
    "    \"bm25_dense\": hybrid,\n",
    "    \"dense_tfidf\": hybrid,\n",
    "    \"dense_bm25\": hybrid,\n",
    "    \n",
    "}\n",
    "\n",
    "# For each dataset, define a configuration for each retrieval method.\n",
    "# Here we create a dictionary keyed by retrieval method for each dataset.\n",
    "for ds in datasets:\n",
    "    ds.setdefault(\"retrieval_config\", {})\n",
    "    #TF-IDF configuration.\n",
    "    ds[\"retrieval_config\"][\"tfidf\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"vectorizer\": tfidf_vectorizer,\n",
    "        \"doc_matrix\": doc_matrix,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"question\"  # or \"sub_questions\" based on your data structure\n",
    "    }\n",
    "    # BM25 configuration.\n",
    "    ds[\"retrieval_config\"][\"bm25\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"bm25\": bm25,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"question\"  # or \"sub_questions\" based on your data structure\n",
    "    }\n",
    "    #Dense retrieval configuration.\n",
    "    ds[\"retrieval_config\"][\"dense\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"all_subqueries\": ds[\"dense\"][\"subqueries\"],\n",
    "        \"subquery_index\": ds[\"dense\"][\"subquery_index\"],\n",
    "        \"faiss_index\": corpus_dense_index,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"sub_questions\"  # or \"question\" based on your data structure\n",
    "    }\n",
    "        # --- Add Hybrid Configurations ---\n",
    "    # Hybrid TF-IDF + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"tfidf_dense\"] = {\n",
    "        \"sparse_func\": tfidf_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"tfidf\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,   # You can choose this value independently.\n",
    "        \"final_k\": 20,           # And choose the final number of results.\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"sparse_dense\"  # or \"dense_sparse\"\n",
    "\n",
    "    }\n",
    "    # Hybrid BM25 + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"bm25_dense\"] = {\n",
    "        \"sparse_func\": bm25_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"bm25\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,\n",
    "        \"final_k\": 20,\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"sparse_dense\"  # or \"dense_sparse\"\n",
    "    }\n",
    "    ds[\"retrieval_config\"][\"dense_tfidf\"] = {\n",
    "        \"sparse_func\": tfidf_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"tfidf\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,   # You can choose this value independently.\n",
    "        \"final_k\": 20,           # And choose the final number of results.\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"dense_sparse\"  # or \"dense_sparse\"\n",
    "\n",
    "    }\n",
    "    # Hybrid BM25 + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"dense_bm25\"] = {\n",
    "        \"sparse_func\": bm25_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"bm25\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,\n",
    "        \"final_k\": 20,\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"dense_sparse\"  # or \"dense_sparse\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'easy_single': loaded 253 ground truth examples.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m origin_question \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     68\u001b[0m subquestions \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m query \u001b[38;5;241m=\u001b[39m [\u001b[43morigin_question\u001b[49m, subquestions]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# if query_type == \"sub_questions\":\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#     question = query[1]\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# elif query_type == \"question\":\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#     question = query[0]\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#     question = query \u001b[39;00m\n\u001b[0;32m     76\u001b[0m retrieval_results \u001b[38;5;241m=\u001b[39m method_func(query, config)\n",
      "Cell \u001b[1;32mIn[87], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m origin_question \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     68\u001b[0m subquestions \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 69\u001b[0m query \u001b[38;5;241m=\u001b[39m [\u001b[43morigin_question\u001b[49m, subquestions]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# if query_type == \"sub_questions\":\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#     question = query[1]\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# elif query_type == \"question\":\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#     question = query[0]\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#     question = query \u001b[39;00m\n\u001b[0;32m     76\u001b[0m retrieval_results \u001b[38;5;241m=\u001b[39m method_func(query, config)\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\nlp\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2185\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2188\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2190\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\nlp\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2254\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2255\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2257\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2258\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\nlp\\lib\\threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\nlp\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assume the following retrieval and generation functions and classes are defined and imported:\n",
    "# - tfidf_retrieval, bm25_retrieval, dense_retrieval_subqueries, hybrid, query_embed_search.\n",
    "# - retrieval_methods dictionary mapping method names to functions.\n",
    "# - CohereGenerator class.\n",
    "# - retrieve_all_subqueries, load_corpus, etc.\n",
    "\n",
    "# For generation, we instantiate our generator.\n",
    "generator = CohereGenerator(model=\"command-r\")\n",
    "\n",
    "# # Example: use the \"tfidf_dense\" hybrid method for generation.\n",
    "# selected_method = \"tfidf_dense\"\n",
    "# # retrieval_methods is assumed to be defined, for example:\n",
    "# # retrieval_methods = {\n",
    "# #    \"tfidf\": tfidf_retrieval,\n",
    "# #    \"bm25\": bm25_retrieval,\n",
    "# #    \"dense\": dense_retrieval_subqueries,\n",
    "# #    \"tfidf_dense\": hybrid,\n",
    "# #    \"bm25_dense\": hybrid,\n",
    "# #    \"dense_tfidf\": hybrid,\n",
    "# #    \"dense_bm25\": hybrid\n",
    "# # }\n",
    "# selected_dataset = \"easy_single\"  # Example dataset name.\n",
    "\n",
    "# # Load the dataset configuration based on the selected method.\n",
    "\n",
    "# test_results = []  # Store results for each dataset.\n",
    "# # For each dataset (datasets list is defined as in your configuration):\n",
    "# ds = next((d for d in datasets if d[\"name\"] == selected_dataset), None)\n",
    "# # Load ground truth generation data.\n",
    "# with open(ds[\"gt_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "#     ground_truth_data = json.load(f)\n",
    "# print(f\"Dataset '{ds['name']}': loaded {len(ground_truth_data)} ground truth examples.\")\n",
    "\n",
    "# # Choose the retrieval configuration for generation.\n",
    "# # Here we use the hybrid method \"tfidf_dense\" (which uses combined query).\n",
    "# retrieval_config = ds[\"retrieval_config\"][selected_method]\n",
    "# retrieval_func = retrieval_methods[selected_method]\n",
    "\n",
    "# predictions = []  # Store generated answers.\n",
    "# references = []   # Store list of reference answers.\n",
    "\n",
    "select_datasets = [\"easy_single\", \"medium_single\"]\n",
    "select_retrieval_methods = [\"tfidf_dense\"]\n",
    "# Filter datasets based on the selected names.\n",
    "datasets = [ds for ds in datasets if ds[\"name\"] in select_datasets]\n",
    "# Filter retrieval methods based on the selected names.\n",
    "retrieval_methods = {k: v for k, v in retrieval_methods.items() if k in select_retrieval_methods}\n",
    "\n",
    "results = []  # Store results for each dataset.\n",
    "for ds in datasets:\n",
    "    # Load ground truth data.\n",
    "    with open(ds[\"gt_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth_data = json.load(f)\n",
    "    print(f\"Dataset '{ds['name']}': loaded {len(ground_truth_data)} ground truth examples.\")\n",
    "    \n",
    "    # For each retrieval method defined in this dataset's config:\n",
    "    for method_name, method_func in retrieval_methods.items():\n",
    "        # Check if a configuration exists for this method.\n",
    "        if method_name in ds[\"retrieval_config\"]:\n",
    "            config = ds[\"retrieval_config\"][method_name]\n",
    "            predictions = []\n",
    "            references = []\n",
    "            for example in ground_truth_data:\n",
    "                query_type = config.get(\"query_type\")\n",
    "                origin_question = example[\"question\"]\n",
    "                subquestions = example[\"sub_questions\"]\n",
    "                query = [origin_question, subquestions]\n",
    "                # if query_type == \"sub_questions\":\n",
    "                #     question = query[1]\n",
    "                # elif query_type == \"question\":\n",
    "                #     question = query[0]\n",
    "                # else:\n",
    "                #     question = query \n",
    "                retrieval_results = method_func(query, config)\n",
    "                # Call the generation function.\n",
    "                # generation_answer returns previous Q&A, and a final answer.\n",
    "                previous_qa, final_answer = generator.generation_answer(\n",
    "                    origin_question, subquestions, retrieval_results, top_k=5, max_tokens=60\n",
    "                )\n",
    "                \n",
    "                predictions.append(final_answer)\n",
    "                # Assume the ground truth contains a list of reference answers under \"list of reference\".\n",
    "                references.append(example[\"list of reference\"])\n",
    "            \n",
    "            passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "\n",
    "            gen_metrics = MetricCollection({\n",
    "        \"bleu\": BLEU(),\n",
    "        \"rouge\": ROUGE(),\n",
    "        \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "    })\n",
    "\n",
    "            gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "            gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": ds[\"name\"],\n",
    "                \"retrieval_method\": method_name,\n",
    "                \"generation_model\": \"cohere\",\n",
    "                \"metrics\": gen_results\n",
    "            })\n",
    "            print(f\"\\nGeneration evaluation metrics for dataset {ds['name']} using {method_name}:\")\n",
    "            print(json.dumps(gen_results, indent=2))\n",
    "\n",
    "            # \n",
    "            \n",
    "\n",
    "\n",
    "# for example in ground_truth_data:\n",
    "#     origin_question = example[\"question\"]\n",
    "#     subquestions = example[\"sub_questions\"]\n",
    "#     # For hybrid retrieval, we combine the main question and the sub_questions.\n",
    "#     # Our hybrid function expects a list: [query_for_sparse, query_for_dense].\n",
    "#     query = [origin_question, subquestions]\n",
    "#     # Get retrieval results using the chosen retrieval method.\n",
    "#     retrieval_results = retrieval_func(query, retrieval_config)\n",
    "    \n",
    "#     # Call the generation function.\n",
    "#     # generation_answer returns previous Q&A, and a final answer.\n",
    "#     previous_qa, final_answer = generator.generation_answer(\n",
    "#         origin_question, subquestions, retrieval_results, top_k=5, max_tokens=60\n",
    "#     )\n",
    "    \n",
    "#     predictions.append(final_answer)\n",
    "#     # Assume the ground truth contains a list of reference answers under \"list of reference\".\n",
    "#     references.append(example[\"list of reference\"])\n",
    "#     passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "#     # save the references for later evaluation.\n",
    "#     # test_results.append({\n",
    "#     #     \"question\": origin_question,\n",
    "#     #     \"sub_questions\": subquestions,\n",
    "#     #     \"retrieval_results\": retrieval_results,\n",
    "#     #     \"previous_qa\": previous_qa,\n",
    "#     #     \"final_answer\": final_answer,\n",
    "#     #     \"references\": example[\"list of reference\"],\n",
    "#     #     \"datasets\": ds[\"name\"]\n",
    "#     # })\n",
    "#     # Save the predictions to a file.\n",
    "\n",
    "#     # Evaluate generation performance using generation metrics.\n",
    "#     gen_metrics = MetricCollection({\n",
    "#         \"bleu\": BLEU(),\n",
    "#         \"rouge\": ROUGE(),\n",
    "#         \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "#     })\n",
    "\n",
    "#     gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "#     gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "\n",
    "    \n",
    "#     # print(f\"\\nGeneration evaluation metrics for dataset {ds['name']}:\")\n",
    "#     # print(json.dumps(gen_results, indent=2))\n",
    "\n",
    "#     # Save the test results to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "253\n",
      "ROUGE Score: 0.03572619228406683\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE TEST RESULTS FOR EVALUATION\n",
    "test_results_path = os.path.join(PM_PATH, f\"test_results_{selected_method}.json\")\n",
    "rouge = ROUGE()\n",
    "\n",
    "# Load the test results for evaluation.\n",
    "with open(test_results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_results = json.load(f)\n",
    "\n",
    "predictions = [result[\"final_answer\"] for result in test_results]\n",
    "# Process references to ensure they are lists of strings\n",
    "references = [result[\"references\"] for result in test_results]\n",
    "passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "\n",
    "passages = []\n",
    "passage = []\n",
    "for ref in references:\n",
    "    for r in ref:\n",
    "        passage.append(r[\"passage\"])\n",
    "    passages.append(passage)        \n",
    "\n",
    "\n",
    "print(len(passages))\n",
    "print(len(predictions))\n",
    "\n",
    "# # Evaluate the test results using the evaluation functions.\n",
    "# #for prediction, reference in zip(predictions, references):\n",
    "# print(predictions)\n",
    "# # print type of predictions and references\n",
    "# print(type(predictions))\n",
    "# print(type(references))\n",
    "# for prediction, reference in zip(predictions, references):\n",
    "#     # Ensure that `reference` is a list of strings as required by ROUGE\n",
    "#     processed_reference = [ref[\"passage\"] if isinstance(ref, dict) else ref for ref in reference]\n",
    "#     rouge.update(prediction, processed_reference)\n",
    "#     total_score = rouge.compute()\n",
    "#     print(f\"ROUGE Score: {total_score}\")\n",
    "# #Update ROUGE with processed references\n",
    "\n",
    "rouge.update(predictions, passages)\n",
    "total_score = rouge.compute()\n",
    "print(f\"ROUGE Score: {total_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results saved to ../performance\\test_results_tfidf_dense.json\n"
     ]
    }
   ],
   "source": [
    "# save the test results to a file.\n",
    "test_results_path = os.path.join(PM_PATH, f\"test_results_{selected_method}.json\")\n",
    "with open(test_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(f\"Test results saved to {test_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv_path: d:\\NLP_Project\\NLP_project\\key.env\n",
      "IX1TJYpIXliOPw5R8q8YaUvKcGvgTXZZRd32ThsV\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent  # 当前 notebook 假设在 performance/ 下\n",
    "dotenv_path = project_root / \"key.env\"\n",
    "print(f\"dotenv_path: {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# 使用环境变量\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_references(refs):\n",
    "    \"\"\"\n",
    "    Process a list of reference lists so that each reference is a string.\n",
    "    If a reference is a dictionary and contains an \"answer\" key, extract it.\n",
    "    Otherwise, assume it is already a string.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for ref_list in refs:\n",
    "        processed_ref_list = []\n",
    "        for ref in ref_list:\n",
    "            if isinstance(ref, dict):\n",
    "                # Adjust the key as needed (e.g., \"answer\", \"text\", etc.)\n",
    "                processed_ref_list.append(ref.get(\"answer\", \"\"))\n",
    "            else:\n",
    "                processed_ref_list.append(ref)\n",
    "        processed.append(processed_ref_list)\n",
    "    return processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
