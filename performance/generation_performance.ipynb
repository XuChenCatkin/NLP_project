{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading passages from ./data/chunked_text_all_together_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from evaluation.eval_utils import *\n",
    "from evaluation.eval_utils import *\n",
    "import json\n",
    "from retrieval import *\n",
    "import numpy as np\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the generation model here\n",
    "from generation.cohere_generation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9251 corpus passages\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Retrieval Method Functions\n",
    "#########################################\n",
    "\n",
    "def tfidf_retrieval(query, config):\n",
    "    \"\"\"\n",
    "    TF-IDF retrieval.\n",
    "    config must include:\n",
    "      - \"vectorizer\": a fitted TfidfVectorizer,\n",
    "      - \"doc_matrix\": document-term matrix,\n",
    "      - \"passages\": list of passages,\n",
    "      - \"chunk_ids\": list of passage identifiers,\n",
    "      - \"top_k\": number of top results.\n",
    "    \"\"\"\n",
    "    vectorizer = config[\"vectorizer\"]\n",
    "    doc_matrix = config[\"doc_matrix\"]\n",
    "    passages = config[\"passages\"]\n",
    "    chunk_ids = config[\"chunk_ids\"]\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    \n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_similarities = (doc_matrix @ query_vec.T).toarray().flatten()\n",
    "    sorted_indices = np.argsort(cosine_similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = [{\"chunk_id\": chunk_ids[i], \"score\": float(cosine_similarities[i])} \n",
    "               for i in sorted_indices]\n",
    "    return results\n",
    "\n",
    "def bm25_retrieval(query, config):\n",
    "    \"\"\"\n",
    "    BM25 retrieval.\n",
    "    config must include:\n",
    "      - \"bm25\": a BM25Okapi object,\n",
    "      - \"passages\": list of passages,\n",
    "      - \"chunk_ids\": list of passage identifiers,\n",
    "      - \"top_k\": number of top results.\n",
    "    \"\"\"\n",
    "    bm25 = config[\"bm25\"]\n",
    "    passages = config[\"passages\"]\n",
    "    chunk_ids = config[\"chunk_ids\"]\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    \n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    sorted_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    results = [{\"chunk_id\": chunk_ids[i], \"score\": float(scores[i])} \n",
    "               for i in sorted_indices]\n",
    "    return results\n",
    "\n",
    "def dense_retrieval_subqueries(queries, config):\n",
    "\n",
    "    if isinstance(queries, str):\n",
    "        queries = [queries]\n",
    "\n",
    "    results = []\n",
    "    top_k = config.get(\"top_k\", 5)\n",
    "    for query in queries:\n",
    "        query_emb = query_embed_search(query, config[\"all_subqueries\"], config[\"subquery_index\"])\n",
    "        query_emb = query_emb.reshape(1, -1)  # Reshape to (1, d)\n",
    "        distances, indices = config[\"faiss_index\"].search(query_emb, top_k)\n",
    "        results.extend([\n",
    "            {\n",
    "                \"sub_query\": query,\n",
    "                \"chunk_id\": config[\"chunk_ids\"][i],\n",
    "                \"passage\": config[\"passages\"][i],\n",
    "                \"score\": float(distances[0][j])\n",
    "            } for j, i in enumerate(indices[0])\n",
    "        ])\n",
    "    return results\n",
    "\n",
    "def query_embed_search(query, all_queries_list, index):\n",
    "    \"\"\"\n",
    "    Given a query, finds its embedding from the precomputed subqueries index.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        position = all_queries_list.index(query)\n",
    "        return index.reconstruct(position)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Query '{query}' not found in the list of all queries.\")\n",
    "    \n",
    "def hybrid(query, config):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval: first retrieve candidates with one method, then re-rank with the other.\n",
    "    \n",
    "    The order is determined by config[\"order\"]:\n",
    "      - \"sparse_dense\" (default): Use query[0] (or query if not a list) for sparse retrieval,\n",
    "                                   then query[1] (or query) for dense retrieval.\n",
    "      - \"dense_sparse\": Use query[0] for dense retrieval,\n",
    "                        then query[1] for sparse retrieval.\n",
    "    \n",
    "    The candidate sets are combined (using the \"mode\": \"intersection\" or \"union\")\n",
    "    and then sorted by the score of the re-ranking model.\n",
    "    \n",
    "    Parameters:\n",
    "      - query (str or list): If a list of at least two elements, the two parts will be used\n",
    "                             in the order specified by config[\"order\"]. Otherwise, the same query is used.\n",
    "      - config (dict): Must contain:\n",
    "          * \"order\": either \"sparse_dense\" (default) or \"dense_sparse\"\n",
    "          * \"sparse_func\": sparse retrieval function (e.g., tfidf_retrieval or bm25_retrieval)\n",
    "          * \"sparse_config\": configuration for the sparse method.\n",
    "          * \"dense_func\": dense retrieval function (e.g., dense_retrieval_subqueries)\n",
    "          * \"dense_config\": configuration for the dense method.\n",
    "          * \"intermediate_k\": candidate set size (default: 30)\n",
    "          * \"final_k\": final result count (default: 5)\n",
    "          * \"mode\": \"intersection\" (default) or \"union\"\n",
    "    \n",
    "    Returns:\n",
    "      A list of final results (dictionaries) from the re-ranking step.\n",
    "    \"\"\"\n",
    "    order = config.get(\"order\", \"sparse_dense\")\n",
    "    intermediate_k = config.get(\"intermediate_k\", 30)\n",
    "    final_k = config.get(\"final_k\", 5)\n",
    "    mode = config.get(\"mode\", \"intersection\")\n",
    "    \n",
    "    # Decide which query part to use for each retrieval step.\n",
    "    if isinstance(query, list) and len(query) >= 2:\n",
    "        query_sparse = query[0]\n",
    "        query_dense = query[1]\n",
    "    else:\n",
    "        print(\"Query should be a list of two queries: [sparse_query, dense_query].\")\n",
    "\n",
    "\n",
    "    if order == \"sparse_dense\":\n",
    "        # Sparse retrieval first.\n",
    "        sparse_config = config[\"sparse_config\"].copy()\n",
    "        sparse_config[\"top_k\"] = intermediate_k\n",
    "        sparse_results = config[\"sparse_func\"](query_sparse, sparse_config)\n",
    "        \n",
    "        # Dense retrieval.\n",
    "        dense_results = config[\"dense_func\"](query_dense, config[\"dense_config\"])\n",
    "        \n",
    "        # Combine: here we choose to filter dense results by the candidate set from sparse.\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in sparse_results)\n",
    "        filtered_dense = [r for r in dense_results if r[\"chunk_id\"] in candidate_ids]\n",
    "        if not filtered_dense:\n",
    "            filtered_dense = dense_results\n",
    "        # Sort by dense score.\n",
    "        sorted_results = sorted(filtered_dense, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    elif order == \"dense_sparse\":\n",
    "        # Dense retrieval first.\n",
    "        dense_results = config[\"dense_func\"](query_dense, config[\"dense_config\"])\n",
    "        # Sparse retrieval.\n",
    "        sparse_config = config[\"sparse_config\"].copy()\n",
    "        sparse_config[\"top_k\"] = intermediate_k\n",
    "        sparse_results = config[\"sparse_func\"](query_sparse, sparse_config)\n",
    "        # Combine: filter sparse results by dense candidate set.\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in dense_results)\n",
    "        filtered_sparse = [r for r in sparse_results if r[\"chunk_id\"] in candidate_ids]\n",
    "        if not filtered_sparse:\n",
    "            filtered_sparse = sparse_results\n",
    "        # Sort by sparse score.\n",
    "        sorted_results = sorted(filtered_sparse, key=lambda x: x[\"score\"], reverse=True)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid order specified.\")\n",
    "    \n",
    "    return sorted_results[:final_k]\n",
    "def hybrid(query, config):\n",
    "\n",
    "    # Check if the query is a list with at least two elements\n",
    "    if isinstance(query, list) and len(query) >= 2:\n",
    "        query_sparse = query[0]\n",
    "        query_dense = query[1]\n",
    "    else:\n",
    "        print(\"Query should be a list of two queries: [sparse_query, dense_query].\")\n",
    "\n",
    "\n",
    "\n",
    "    intermediate_k = config.get(\"intermediate_k\")\n",
    "    final_k = config.get(\"final_k\")\n",
    "    mode = config.get(\"mode\", \"intersection\")\n",
    "    \n",
    "    # Step 1: Sparse retrieval using query_sparse.\n",
    "    sparse_config = config[\"sparse_config\"].copy()\n",
    "    sparse_config[\"top_k\"] = intermediate_k\n",
    "    sparse_results = config[\"sparse_func\"](query_sparse, sparse_config)\n",
    "    \n",
    "    # Step 2: Dense retrieval using query_dense.\n",
    "    dense_results = config[\"dense_func\"](query_dense, config[\"dense_config\"])\n",
    "    \n",
    "    # Step 3: Combine candidate sets.\n",
    "    if mode == \"intersection\":\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in sparse_results)\n",
    "        filtered_dense = [r for r in dense_results if r[\"chunk_id\"] in candidate_ids]\n",
    "        if not filtered_dense:\n",
    "            filtered_dense = dense_results  # fallback if intersection is empty\n",
    "    elif mode == \"union\":\n",
    "        candidate_ids = set(r[\"chunk_id\"] for r in sparse_results).union(\n",
    "                        set(r[\"chunk_id\"] for r in dense_results))\n",
    "        dense_dict = {r[\"chunk_id\"]: r for r in dense_results}\n",
    "        filtered_dense = []\n",
    "        for cid in candidate_ids:\n",
    "            if cid in dense_dict:\n",
    "                filtered_dense.append(dense_dict[cid])\n",
    "            else:\n",
    "                filtered_dense.append({\"chunk_id\": cid, \"score\": 0.0})\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'intersection' or 'union'.\")\n",
    "    \n",
    "    # Step 4: Sort by dense score (descending) and select top final_k.\n",
    "    sorted_dense = sorted(filtered_dense, key=lambda x: x[\"score\"], reverse=True)\n",
    "    #final_results = sorted_dense[:final_k]\n",
    "    final_results = sorted_dense\n",
    "    \n",
    "    # ***** String Constraint Enforcement After Hybrid Retrieval *****\n",
    "    # Ensure that query_dense is a string.\n",
    "    query_dense = str(query_dense)\n",
    "    # For each candidate, ensure the passage is a string.\n",
    "    for candidate in final_results:\n",
    "        candidate[\"passage\"] = str(candidate.get(\"passage\", \"\"))\n",
    "    \n",
    "    # Step 5 (Optional): Cross-Encoder re-ranking.\n",
    "    if config.get(\"rerank\") and \"cross_encoder_model\" in config:\n",
    "        cross_encoder = config[\"cross_encoder_model\"]\n",
    "        # Before passing sentence pairs to cross_encoder.predict, ensure each passage is a string.\n",
    "        sentence_pairs = []\n",
    "        for candidate in final_results:\n",
    "            passage = candidate.get(\"passage\", \"\")\n",
    "            if not isinstance(passage, str):\n",
    "                passage = str(passage)\n",
    "            sentence_pairs.append([query_dense, passage])\n",
    "        try:\n",
    "            cross_scores = cross_encoder.predict(sentence_pairs)\n",
    "        except Exception as e:\n",
    "            print(\"Error during cross encoder prediction:\", e)\n",
    "            cross_scores = [candidate[\"score\"] for candidate in final_results]  # fallback\n",
    "        for idx, candidate in enumerate(final_results):\n",
    "            candidate[\"cross_encoder_score\"] = float(cross_scores[idx])\n",
    "        final_results = sorted(final_results, key=lambda x: x[\"cross_encoder_score\"], reverse=True)\n",
    "    \n",
    "    return final_results[:final_k]\n",
    "\n",
    "#########################################\n",
    "# Dataset Configuration\n",
    "#########################################\n",
    "\n",
    "# Global corpus (for all retrieval methods that use it).\n",
    "def load_corpus(corpus_file):\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    passages = [entry[\"passage\"] for entry in data if \"passage\" in entry]\n",
    "    chunk_ids = [entry[\"chunk_id\"] for entry in data if \"chunk_id\" in entry]\n",
    "    print(f\"Loaded {len(passages)} corpus passages\")\n",
    "    return passages, chunk_ids\n",
    "\n",
    "# A helper function to load subqueries from a ground truth file.\n",
    "def retrieve_all_subqueries(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "    subqueries = []\n",
    "    for item in qa_data:\n",
    "        subqueries.extend(item[\"sub_questions\"])\n",
    "    return subqueries\n",
    "\n",
    "# Paths for the corpus and QA sets.\n",
    "CORPUS_FILE = \"../data/chunked_text_all_together_cleaned.json\"\n",
    "QA_PATH = \"../data/QA_set\"\n",
    "QA_EMBEDDED_PATH = \"../embedding/BAAI/bge-base-en-v1.5\"\n",
    "PM_PATH = \"../performance\"\n",
    "\n",
    "# CORPUS_FILE = os.path.join(project_root, \"data\", \"chunked_text_all_together_cleaned.json\")\n",
    "# QA_PATH = os.path.join(project_root, \"data\", \"QA_set\")\n",
    "# QA_EMBEDDED_PATH = os.path.join(project_root, \"data\", \"QA_set_embedded\")\n",
    "# Load the corpus and build common indexes.\n",
    "\n",
    "passages, chunk_ids = load_corpus(CORPUS_FILE)\n",
    "\n",
    "# Build TF-IDF index for the corpus.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_matrix = tfidf_vectorizer.fit_transform(passages)\n",
    "\n",
    "# Build BM25 index for the corpus.\n",
    "tokenized_passages = [word_tokenize(p.lower()) for p in passages]\n",
    "bm25 = BM25Okapi(tokenized_passages)\n",
    "\n",
    "# Load global dense corpus index (FAISS).\n",
    "corpus_dense_index = faiss.read_index(\"../hp_all_bge.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"easy_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"easy_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"easy_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"easy_single_labeled_embeddings.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"medium_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"medium_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"medium_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_medium_single_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"medium_multi\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"medium_multi_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"medium_multi_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_medium_multi_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"hard_single\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"hard_single_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"hard_single_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_hard_single_labeled.index\"))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"hard_multi\",\n",
    "        \"gt_path\": os.path.join(QA_PATH, \"hard_multi_labeled.json\"),\n",
    "        \"dense\": {\n",
    "            \"subqueries\": retrieve_all_subqueries(os.path.join(QA_PATH, \"hard_multi_labeled.json\")),\n",
    "            \"subquery_index\": faiss.read_index(os.path.join(QA_EMBEDDED_PATH, \"bge_hard_multi_labeled.index\"))\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dictionary mapping retrieval method names to their functions.\n",
    "retrieval_methods = {\n",
    "    \"tfidf\": tfidf_retrieval,\n",
    "    \"bm25\": bm25_retrieval,\n",
    "    \"dense\": dense_retrieval_subqueries,\n",
    "    \"tfidf_dense\": hybrid,\n",
    "    \"bm25_dense\": hybrid,\n",
    "    \"dense_tfidf\": hybrid,\n",
    "    \"dense_bm25\": hybrid,\n",
    "    \n",
    "}\n",
    "\n",
    "# For each dataset, define a configuration for each retrieval method.\n",
    "# Here we create a dictionary keyed by retrieval method for each dataset.\n",
    "for ds in datasets:\n",
    "    ds.setdefault(\"retrieval_config\", {})\n",
    "    #TF-IDF configuration.\n",
    "    ds[\"retrieval_config\"][\"tfidf\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"vectorizer\": tfidf_vectorizer,\n",
    "        \"doc_matrix\": doc_matrix,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"question\"  # or \"sub_questions\" based on your data structure\n",
    "    }\n",
    "    # BM25 configuration.\n",
    "    ds[\"retrieval_config\"][\"bm25\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"bm25\": bm25,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"question\"  # or \"sub_questions\" based on your data structure\n",
    "    }\n",
    "    #Dense retrieval configuration.\n",
    "    ds[\"retrieval_config\"][\"dense\"] = {\n",
    "        \"passages\": passages,\n",
    "        \"chunk_ids\": chunk_ids,\n",
    "        \"all_subqueries\": ds[\"dense\"][\"subqueries\"],\n",
    "        \"subquery_index\": ds[\"dense\"][\"subquery_index\"],\n",
    "        \"faiss_index\": corpus_dense_index,\n",
    "        \"top_k\": 20,\n",
    "        \"query_type\": \"sub_questions\"  # or \"question\" based on your data structure\n",
    "    }\n",
    "        # --- Add Hybrid Configurations ---\n",
    "    # Hybrid TF-IDF + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"tfidf_dense\"] = {\n",
    "        \"sparse_func\": tfidf_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"tfidf\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,   # You can choose this value independently.\n",
    "        \"final_k\": 20,           # And choose the final number of results.\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"sparse_dense\"  # or \"dense_sparse\"\n",
    "\n",
    "    }\n",
    "    # Hybrid BM25 + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"bm25_dense\"] = {\n",
    "        \"sparse_func\": bm25_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"bm25\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,\n",
    "        \"final_k\": 20,\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"sparse_dense\"  # or \"dense_sparse\"\n",
    "    }\n",
    "    ds[\"retrieval_config\"][\"dense_tfidf\"] = {\n",
    "        \"sparse_func\": tfidf_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"tfidf\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,   # You can choose this value independently.\n",
    "        \"final_k\": 20,           # And choose the final number of results.\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"dense_sparse\"  # or \"dense_sparse\"\n",
    "\n",
    "    }\n",
    "    # Hybrid BM25 + Dense configuration.\n",
    "    ds[\"retrieval_config\"][\"dense_bm25\"] = {\n",
    "        \"sparse_func\": bm25_retrieval,\n",
    "        \"sparse_config\": ds[\"retrieval_config\"][\"bm25\"],\n",
    "        \"dense_func\": dense_retrieval_subqueries,\n",
    "        \"dense_config\": ds[\"retrieval_config\"][\"dense\"],\n",
    "        \"intermediate_k\": 1000,\n",
    "        \"final_k\": 20,\n",
    "        \"mode\": \"intersection\" , # or \"union\"\n",
    "        \"query_type\": \"combine\",  # or \"sub_questions\" based on your data structure\n",
    "        \"order\": \"dense_sparse\"  # or \"dense_sparse\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere API key loaded from key.env\n",
      "Dataset 'easy_single': loaded 253 ground truth examples.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retrieval_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m query \u001b[38;5;241m=\u001b[39m [origin_question, subquestions]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Get retrieval results using the chosen retrieval method.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m retrieval_results \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_func\u001b[49m(query, retrieval_config)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Call the generation function.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# generation_answer returns previous Q&A, and a final answer.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m previous_qa, final_answer \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgeneration_answer(\n\u001b[1;32m     49\u001b[0m     origin_question, subquestions, retrieval_results, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retrieval_func' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assume the following retrieval and generation functions and classes are defined and imported:\n",
    "# - tfidf_retrieval, bm25_retrieval, dense_retrieval_subqueries, hybrid, query_embed_search.\n",
    "# - retrieval_methods dictionary mapping method names to functions.\n",
    "# - CohereGenerator class.\n",
    "# - retrieve_all_subqueries, load_corpus, etc.\n",
    "\n",
    "# For generation, we instantiate our generator.\n",
    "generator = CohereGenerator(model=\"command-r\")\n",
    "\n",
    "# Example: use the \"tfidf_dense\" hybrid method for generation.\n",
    "selected_method = \"dense\"\n",
    "# retrieval_methods is assumed to be defined, for example:\n",
    "# retrieval_methods = {\n",
    "#    \"tfidf\": tfidf_retrieval,\n",
    "#    \"bm25\": bm25_retrieval,\n",
    "#    \"dense\": dense_retrieval_subqueries,\n",
    "#    \"tfidf_dense\": hybrid,\n",
    "#    \"bm25_dense\": hybrid,\n",
    "#    \"dense_tfidf\": hybrid,\n",
    "#    \"dense_bm25\": hybrid\n",
    "# }\n",
    "selected_dataset = \"easy_single\"  # Example dataset name.\n",
    "\n",
    "# Load the dataset configuration based on the selected method.\n",
    "\n",
    "test_results = []  # Store results for each dataset.\n",
    "# For each dataset (datasets list is defined as in your configuration):\n",
    "ds = next((d for d in datasets if d[\"name\"] == selected_dataset), None)\n",
    "# Load ground truth generation data.\n",
    "with open(ds[\"gt_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "print(f\"Dataset '{ds['name']}': loaded {len(ground_truth_data)} ground truth examples.\")\n",
    "\n",
    "retrieval_func = retrieval_methods[selected_method]\n",
    "retrieval_config = ds[\"retrieval_config\"][selected_method]\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in ground_truth_data:\n",
    "    origin_question = example[\"question\"]\n",
    "    subquestions = example[\"sub_questions\"]\n",
    "    # For hybrid retrieval, we combine the main question and the sub_questions.\n",
    "    # Our hybrid function expects a list: [query_for_sparse, query_for_dense].\n",
    "    query = [origin_question, subquestions]\n",
    "    # Get retrieval results using the chosen retrieval method.\n",
    "    retrieval_results = retrieval_func(query, retrieval_config)\n",
    "    \n",
    "    # Call the generation function.\n",
    "    # generation_answer returns previous Q&A, and a final answer.\n",
    "    previous_qa, final_answer = generator.generation_answer(\n",
    "        origin_question, subquestions, retrieval_results, top_k=5, max_tokens=60\n",
    "    )\n",
    "    \n",
    "    predictions.append(final_answer)\n",
    "    # Assume the ground truth contains a list of reference answers under \"list of reference\".\n",
    "    references.append(example[\"list of reference\"])\n",
    "    \n",
    "    # save the references for later evaluation.\n",
    "    test_results.append({\n",
    "        \"question\": origin_question,\n",
    "        \"sub_questions\": subquestions,\n",
    "        \"retrieval_results\": retrieval_results,\n",
    "        \"previous_qa\": previous_qa,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"references\": example[\"list of reference\"],\n",
    "        \"datasets\": ds[\"name\"]\n",
    "    })\n",
    "    # Save the predictions to a file.\n",
    "\n",
    "    # Evaluate generation performance using generation metrics.\n",
    "# passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "# gen_metrics = MetricCollection({\n",
    "#     \"bleu\": BLEU(),\n",
    "#     \"rouge\": ROUGE(),\n",
    "#     \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "# })\n",
    "\n",
    "# gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "    \n",
    "# gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "predictions = [result[\"final_answer\"] for result in test_results]\n",
    "# Process references to ensure they are lists of strings\n",
    "references = [result[\"references\"] for result in test_results]\n",
    "# passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "\n",
    "passages = []\n",
    "passage = []\n",
    "for ref in references:\n",
    "    for r in ref:\n",
    "        passage.append(r[\"passage\"])\n",
    "    passages.append(passage)    \n",
    "    \n",
    "gen_metrics = MetricCollection({\n",
    "        \"bleu\": BLEU(),\n",
    "        \"rouge\": ROUGE(),\n",
    "        \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "    })\n",
    "\n",
    "gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "print(f\"\\nGeneration evaluation metrics for dataset {ds['name']}:\")\n",
    "\n",
    "# print(json.dumps(gen_results, indent=2))\n",
    "\n",
    "\n",
    "# save_path = os.path.join(PM_PATH, f\"test_results_{selected_method}_2.json\")\n",
    "# with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(test_results, f, indent=2)\n",
    "#     # Save the test results to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere API key loaded from key.env\n",
      "Dataset 'medium_multi': loaded 35 ground truth examples.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'i' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\NLP_Project\\NLP_project\\evaluation\\eval_utils.py:270\u001b[0m, in \u001b[0;36mBLEU.update\u001b[1;34m(self, predictions, references)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, pred, refs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(predictions, references)):\n\u001b[0;32m    271\u001b[0m         ref_tokens \u001b[38;5;241m=\u001b[39m [ref\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs]\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 81\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Compute generation evaluation metrics.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m gen_metrics \u001b[38;5;241m=\u001b[39m MetricCollection({\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m: BLEU(),\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m: ROUGE(),\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: BERTScore(model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m })\n\u001b[1;32m---> 81\u001b[0m \u001b[43mgen_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeneration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m gen_results \u001b[38;5;241m=\u001b[39m gen_metrics\u001b[38;5;241m.\u001b[39mcompute(metric_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGeneration evaluation metrics for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with generation model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\NLP_Project\\NLP_project\\evaluation\\eval_utils.py:419\u001b[0m, in \u001b[0;36mMetricCollection.update\u001b[1;34m(self, predictions, references, metric_type)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mmetric_type \u001b[38;5;241m==\u001b[39m metric_type \u001b[38;5;129;01mor\u001b[39;00m metric_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 419\u001b[0m         \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NLP_Project\\NLP_project\\evaluation\\eval_utils.py:275\u001b[0m, in \u001b[0;36mBLEU.update\u001b[1;34m(self, predictions, references)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bleu_scores\u001b[38;5;241m.\u001b[39mappend(sentence_bleu(ref_tokens, gen_tokens))\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError calculating BLEU for index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, References: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrefs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Evaluation of the generation model.\n",
    "########################\n",
    "\n",
    "generator = CohereGenerator(model=\"command-r\")\n",
    "select_datasets = [ \"medium_multi\"]\n",
    "select_retrieval_methods = [\"tfidf_dense\"]\n",
    "# Filter datasets based on the selected names.\n",
    "datasets = [ds for ds in datasets if ds[\"name\"] in select_datasets]\n",
    "# Filter retrieval methods based on the selected names.\n",
    "retrieval_methods = {k: v for k, v in retrieval_methods.items() if k in select_retrieval_methods}\n",
    "\n",
    "\n",
    "results = []  # This list will hold the overall performance for each dataset/model combination.\n",
    "\n",
    "# Loop over each dataset.\n",
    "for ds in datasets:\n",
    "    # Load ground truth data.\n",
    "    with open(ds[\"gt_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth_data = json.load(f)\n",
    "    print(f\"Dataset '{ds['name']}': loaded {len(ground_truth_data)} ground truth examples.\")\n",
    "    \n",
    "    # Loop over each generation model.\n",
    "    for method_name, method_func in retrieval_methods.items():\n",
    "        # Check if a configuration exists for this method.\n",
    "        if method_name in ds[\"retrieval_config\"]:\n",
    "            config = ds[\"retrieval_config\"][method_name]\n",
    "            predictions = []\n",
    "            references = []\n",
    "            test_results = []  # Store detailed results for each example.\n",
    "\n",
    "            # Process each example in the ground truth.\n",
    "            for example in ground_truth_data:\n",
    "                origin_question = example[\"question\"]\n",
    "                subquestions = example[\"sub_questions\"]\n",
    "                # For hybrid retrieval, we assume the query is a two-element list:\n",
    "                # [query_for_sparse, query_for_dense]\n",
    "                query = [origin_question, subquestions]\n",
    "                query_type = config.get(\"query_type\")\n",
    "                if query_type == \"sub_questions\":\n",
    "                    question = query[1]\n",
    "                elif query_type == \"question\":\n",
    "                    question = query[0]\n",
    "                else:\n",
    "                    question = query \n",
    "                # Retrieve candidates using the model-specific retrieval function.\n",
    "                retrieval_results = method_func(question, config)\n",
    "                \n",
    "                # Generate an answer using the generation function.\n",
    "                # generation_answer is expected to return previous Q&A and a final answer.\n",
    "                previous_qa, final_answer = generator.generation_answer(\n",
    "                    origin_question, subquestions, retrieval_results, top_k=5, max_tokens=60\n",
    "                )\n",
    "                \n",
    "                predictions.append(final_answer)\n",
    "                # Assume the ground truth contains a list of reference answers under the key \"list of reference\".\n",
    "                references.append(example[\"list of reference\"])\n",
    "                \n",
    "                # Save the details for later inspection.\n",
    "                test_results.append({\n",
    "                    \"question\": origin_question,\n",
    "                    \"sub_questions\": subquestions,\n",
    "                    \"retrieval_results\": retrieval_results,\n",
    "                    \"previous_qa\": previous_qa,\n",
    "                    \"final_answer\": final_answer,\n",
    "                    \"references\": example[\"list of reference\"],\n",
    "                    \"dataset\": ds[\"name\"],\n",
    "                    \"generation_model\": method_name,\n",
    "                })\n",
    "\n",
    "            # Process the references to extract passage texts.\n",
    "            passages = []\n",
    "            for ref in references:\n",
    "                p = []\n",
    "                for r in ref:\n",
    "                    p.append(r[\"passage\"])\n",
    "                passages.append(p)\n",
    "            \n",
    "            # Compute generation evaluation metrics.\n",
    "            gen_metrics = MetricCollection({\n",
    "                \"bleu\": BLEU(),\n",
    "                \"rouge\": ROUGE(),\n",
    "                \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "            })\n",
    "            gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "            gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "            \n",
    "            print(f\"\\nGeneration evaluation metrics for dataset '{ds['name']}' with generation model '{gen_model_name}':\")\n",
    "            print(gen_results)\n",
    "            \n",
    "            # Append the overall performance for this dataset and generation model.\n",
    "            results.append({\n",
    "                \"data_set\": ds[\"name\"],\n",
    "                \"generation_model\": method_name,\n",
    "                \"performance\": gen_results\n",
    "            })\n",
    "\n",
    "print(\"\\nOverall results:\")\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI-NB\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\MSI-NB\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\MSI-NB\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.008386756880977267, 'rouge': 0.03572619228406683, 'bertscore': {'precision': 0.5920164585113525, 'recall': 0.3643978536128998, 'f1': 0.42138683795928955}}\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE TEST RESULTS FOR EVALUATION\n",
    "test_results_path = os.path.join(PM_PATH, f\"test_results_{selected_method}.json\")\n",
    "rouge = ROUGE()\n",
    "\n",
    "# Load the test results for evaluation.\n",
    "with open(test_results_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_results = json.load(f)\n",
    "\n",
    "predictions = [result[\"final_answer\"] for result in test_results]\n",
    "# Process references to ensure they are lists of strings\n",
    "references = [result[\"references\"] for result in test_results]\n",
    "passages = [[r[\"passage\"] for r in ref] for ref in references]\n",
    "\n",
    "passages = []\n",
    "passage = []\n",
    "for ref in references:\n",
    "    for r in ref:\n",
    "        passage.append(r[\"passage\"])\n",
    "    passages.append(passage)        \n",
    "\n",
    "\n",
    "print(len(passages))\n",
    "print(len(predictions))\n",
    "\n",
    "# # Evaluate the test results using the evaluation functions.\n",
    "# #for prediction, reference in zip(predictions, references):\n",
    "# print(predictions)\n",
    "# # print type of predictions and references\n",
    "# print(type(predictions))\n",
    "# print(type(references))\n",
    "# for prediction, reference in zip(predictions, references):\n",
    "#     # Ensure that `reference` is a list of strings as required by ROUGE\n",
    "#     processed_reference = [ref[\"passage\"] if isinstance(ref, dict) else ref for ref in reference]\n",
    "#     rouge.update(prediction, processed_reference)\n",
    "#     total_score = rouge.compute()\n",
    "#     print(f\"ROUGE Score: {total_score}\")\n",
    "# #Update ROUGE with processed references\n",
    "\n",
    "#rouge.update(predictions, passages)\n",
    "#total_score = rouge.compute()\n",
    "\n",
    "gen_metrics = MetricCollection({\n",
    "        \"bleu\": BLEU(),\n",
    "        \"rouge\": ROUGE(),\n",
    "        \"bertscore\": BERTScore(model_name_or_path=\"bert-base-uncased\", batch_size=16, device=\"cuda\")\n",
    "    })\n",
    "\n",
    "gen_metrics.update(predictions, passages, metric_type=\"generation\")\n",
    "gen_results = gen_metrics.compute(metric_type=\"generation\")\n",
    "print(gen_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results saved to ../performance\\test_results_tfidf_dense.json\n"
     ]
    }
   ],
   "source": [
    "# save the test results to a file.\n",
    "test_results_path = os.path.join(PM_PATH, f\"test_results_{selected_method}.json\")\n",
    "with open(test_results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(f\"Test results saved to {test_results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
